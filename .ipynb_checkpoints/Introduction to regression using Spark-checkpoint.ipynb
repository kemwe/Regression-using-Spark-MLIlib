{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In the following notebook i will use the famous boston housing dataset from kaggle.\n",
    "\n",
    "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive.\n",
    "The dataset is small in size with only 506 cases. It contains 14 features described as follows:\n",
    "CRIM: per capita crime rate by town\n",
    "ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "INDUS: proportion of non-retail business acres per town.\n",
    "CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "NOX: nitric oxides concentration (parts per 10 million)\n",
    "RM: average number of rooms per dwelling\n",
    "AGE: proportion of owner-occupied units built prior to 1940\n",
    "DIS: weighted distances to five Boston employment centres\n",
    "RAD: index of accessibility to radial highways\n",
    "TAX: full-value property-tax rate per $10,000\n",
    "PTRATIO: pupil-teacher ratio by town\n",
    "B: 1000(Bk — 0.63)² where Bk is the proportion of blacks by town\n",
    "LSTAT: % lower status of the population\n",
    "MEDV: Median value of owner-occupied homes in $1000's\n",
    "The goal is to use the 13 features to predict the value of MEDV (which represents the housing price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing SparkSession. It's mostly used when dealing with dataframes \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio|     b|lstat|medv|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|392.52|20.45|15.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2| 390.5|15.71|21.7|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26|20.4|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|380.02|10.26|18.2|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0|395.62| 8.47|19.9|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307|   21.0|386.85| 6.58|23.1|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|386.75|14.67|17.5|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|288.99|11.69|20.2|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|390.95|11.28|18.2|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the spark session\n",
    "spark = SparkSession.builder.appName(\"introduction to regression\").getOrCreate()\n",
    "#importing the dataset\n",
    "data=spark.read.csv('boston_housing.csv',header=True,inferSchema=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <td>506</td>\n",
       "      <td>3.6135235573122535</td>\n",
       "      <td>8.601545105332491</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>88.9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zn</th>\n",
       "      <td>506</td>\n",
       "      <td>11.363636363636363</td>\n",
       "      <td>23.32245299451514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indus</th>\n",
       "      <td>506</td>\n",
       "      <td>11.136778656126504</td>\n",
       "      <td>6.860352940897589</td>\n",
       "      <td>0.46</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chas</th>\n",
       "      <td>506</td>\n",
       "      <td>0.0691699604743083</td>\n",
       "      <td>0.2539940413404101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nox</th>\n",
       "      <td>506</td>\n",
       "      <td>0.5546950592885372</td>\n",
       "      <td>0.11587767566755584</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rm</th>\n",
       "      <td>506</td>\n",
       "      <td>6.284634387351787</td>\n",
       "      <td>0.7026171434153232</td>\n",
       "      <td>3.561</td>\n",
       "      <td>8.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>506</td>\n",
       "      <td>68.57490118577078</td>\n",
       "      <td>28.148861406903595</td>\n",
       "      <td>2.9</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis</th>\n",
       "      <td>506</td>\n",
       "      <td>3.795042687747034</td>\n",
       "      <td>2.10571012662761</td>\n",
       "      <td>1.1296</td>\n",
       "      <td>12.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rad</th>\n",
       "      <td>506</td>\n",
       "      <td>9.549407114624506</td>\n",
       "      <td>8.707259384239366</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax</th>\n",
       "      <td>506</td>\n",
       "      <td>408.2371541501976</td>\n",
       "      <td>168.53711605495903</td>\n",
       "      <td>187</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptratio</th>\n",
       "      <td>506</td>\n",
       "      <td>18.455533596837967</td>\n",
       "      <td>2.1649455237144455</td>\n",
       "      <td>12.6</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>506</td>\n",
       "      <td>356.67403162055257</td>\n",
       "      <td>91.29486438415782</td>\n",
       "      <td>0.32</td>\n",
       "      <td>396.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstat</th>\n",
       "      <td>506</td>\n",
       "      <td>12.653063241106723</td>\n",
       "      <td>7.141061511348571</td>\n",
       "      <td>1.73</td>\n",
       "      <td>37.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medv</th>\n",
       "      <td>506</td>\n",
       "      <td>22.532806324110698</td>\n",
       "      <td>9.197104087379815</td>\n",
       "      <td>5.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                   1                    2        3        4\n",
       "summary  count                mean               stddev      min      max\n",
       "crim       506  3.6135235573122535    8.601545105332491  0.00632  88.9762\n",
       "zn         506  11.363636363636363    23.32245299451514      0.0    100.0\n",
       "indus      506  11.136778656126504    6.860352940897589     0.46    27.74\n",
       "chas       506  0.0691699604743083   0.2539940413404101        0        1\n",
       "nox        506  0.5546950592885372  0.11587767566755584    0.385    0.871\n",
       "rm         506   6.284634387351787   0.7026171434153232    3.561     8.78\n",
       "age        506   68.57490118577078   28.148861406903595      2.9    100.0\n",
       "dis        506   3.795042687747034     2.10571012662761   1.1296  12.1265\n",
       "rad        506   9.549407114624506    8.707259384239366        1       24\n",
       "tax        506   408.2371541501976   168.53711605495903      187      711\n",
       "ptratio    506  18.455533596837967   2.1649455237144455     12.6     22.0\n",
       "b          506  356.67403162055257    91.29486438415782     0.32    396.9\n",
       "lstat      506  12.653063241106723    7.141061511348571     1.73    37.97\n",
       "medv       506  22.532806324110698    9.197104087379815      5.0     50.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you use toPandas() function to apply some of the pandas methods in spark\n",
    "data.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crim       0\n",
       "zn         0\n",
       "indus      0\n",
       "chas       0\n",
       "nox        0\n",
       "rm         0\n",
       "age        0\n",
       "dis        0\n",
       "rad        0\n",
       "tax        0\n",
       "ptratio    0\n",
       "b          0\n",
       "lstat      0\n",
       "medv       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if there are anymissing values in the dataset\n",
    "data.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crim',\n",
       " 'zn',\n",
       " 'indus',\n",
       " 'chas',\n",
       " 'nox',\n",
       " 'rm',\n",
       " 'age',\n",
       " 'dis',\n",
       " 'rad',\n",
       " 'tax',\n",
       " 'ptratio',\n",
       " 'b',\n",
       " 'lstat']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns=data.columns[:-1]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between target value(MEDV) and crim is -0.38830460858681154\n",
      "The correlation between target value(MEDV) and zn is 0.3604453424505433\n",
      "The correlation between target value(MEDV) and indus is -0.4837251600283728\n",
      "The correlation between target value(MEDV) and chas is 0.1752601771902987\n",
      "The correlation between target value(MEDV) and nox is -0.4273207723732821\n",
      "The correlation between target value(MEDV) and rm is 0.6953599470715401\n",
      "The correlation between target value(MEDV) and age is -0.3769545650045961\n",
      "The correlation between target value(MEDV) and dis is 0.249928734085904\n",
      "The correlation between target value(MEDV) and rad is -0.38162623063977735\n",
      "The correlation between target value(MEDV) and tax is -0.46853593356776674\n",
      "The correlation between target value(MEDV) and ptratio is -0.5077866855375622\n",
      "The correlation between target value(MEDV) and b is 0.3334608196570661\n",
      "The correlation between target value(MEDV) and lstat is -0.7376627261740145\n"
     ]
    }
   ],
   "source": [
    "#checking the correlation between the independent values and the target values\n",
    "import pyspark.ml.stat \n",
    "for i in feature_columns:\n",
    "    print (\"The correlation between target value(MEDV) and {} is {}\" .format(i,data.stat.corr(\"medv\",i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "from pyspark.ml.feature import VectorAssembler #for compressing independent variables to features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crim',\n",
       " 'zn',\n",
       " 'indus',\n",
       " 'chas',\n",
       " 'nox',\n",
       " 'rm',\n",
       " 'age',\n",
       " 'dis',\n",
       " 'rad',\n",
       " 'tax',\n",
       " 'ptratio',\n",
       " 'b',\n",
       " 'lstat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns=data.columns[:-1]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|            features|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|[0.02729,0.0,7.07...|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|[0.06905,0.0,2.18...|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the feature column\n",
    "assembler=VectorAssembler(inputCols=feature_columns,outputCol=\"features\")\n",
    "new_data=assembler.transform(data)\n",
    "new_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the description of the dataset above,we can see some columns like crim and b have a huge margin between the max and minimum values\n",
    "# we will have to normalize the data first before using it\n",
    "from pyspark.ml.feature import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+--------------------+\n",
      "|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio|     b|lstat|medv|            features|        new_features|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+--------------------+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|[0.0,0.18,0.06781...|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|[2.35922539178427...|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|[0.02729,0.0,7.07...|[2.35697744000553...|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|[2.92795719180468...|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|[0.06905,0.0,2.18...|[7.05070075400798...|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler=MinMaxScaler(inputCol=\"features\",outputCol=\"new_features\")\n",
    "scale_model=scaler.fit(new_data)\n",
    "scaled_data=scale_model.transform(new_data)\n",
    "scaled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total of original dataset is 506.000000\n",
      "The total of training dataset is 360.000000\n",
      "The total of testing datset is 146.000000\n"
     ]
    }
   ],
   "source": [
    "#splitting the dataset to training and testing\n",
    "splits=scaled_data.randomSplit([0.7,0.3])\n",
    "train=splits[0]\n",
    "test=splits[1]\n",
    "\n",
    "#checking the count of dataset before and after split\n",
    "print(\"The total of original dataset is %f\" %scaled_data.count())\n",
    "print(\"The total of training dataset is %f\" %train.count())\n",
    "print(\"The total of testing datset is %f\" %test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import linear regression module\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "lr=LinearRegression(featuresCol=\"new_features\",labelCol=\"medv\")\n",
    "lrmodel=lr.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model coefficints are :\n",
      " [-9.109180691897778,5.586749495261485,1.6522368830353293,3.87107002624736,-7.894222696139255,21.690027650559674,-0.8429216569248291,-15.55728729813393,6.128738952460953,-7.2775018015055615,-8.752027693264601,3.5134170964208917,-15.454307393049046] \n",
      "\n",
      "The model intercept is: \n",
      "24.922439951391826\n"
     ]
    }
   ],
   "source": [
    "print(\"The model coefficints are :\\n {} \\n\" .format(lrmodel.coefficients))\n",
    "print(\"The model intercept is: \\n{}\".format(lrmodel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.587831\n",
      "Mean Absolute Error: 3.199425\n",
      "R2: 0.743353\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "print(\"Mean Squared Error: %f\" %lrmodel.summary.rootMeanSquaredError)\n",
    "print('Mean Absolute Error: %f' %lrmodel.summary.meanAbsoluteError)\n",
    "print(\"R2: %f\" %lrmodel.summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the above we notice the r2 is 0.7841% . TGhis shows that 74.41% variability in target variable(\"medv\")of the model is likely to be explained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|        new_features|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[8.99180711494721...|31.6| 33.47157073717146|\n",
      "|[9.76735047861141...|50.0| 46.62455787362408|\n",
      "|[1.28807636921618...|32.9|31.218334038130077|\n",
      "|[1.48252419807692...|33.0|23.562667994984647|\n",
      "|[1.49825986052808...|20.1|21.336189820241255|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predicting the values\n",
    "predictions=lrmodel.transform(test)\n",
    "predictions.select('new_features',\"medv\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.040046283328977"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the performance of the model using test data\n",
    "evaluator=RegressionEvaluator(labelCol=\"medv\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above r2,we notice that the model has 61.97% variability when exposed to new data(test data). which is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "dtr=DecisionTreeRegressor(featuresCol=\"new_features\",labelCol=\"medv\")\n",
    "dtrmodel=dtr.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|        new_features|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[8.99180711494721...|31.6| 32.63181818181818|\n",
      "|[9.76735047861141...|50.0|48.857142857142854|\n",
      "|[1.28807636921618...|32.9| 32.63181818181818|\n",
      "|[1.48252419807692...|33.0| 32.63181818181818|\n",
      "|[1.49825986052808...|20.1|        20.9765625|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Root Mean Squared Error (RMSE) on test data is : 4.135875\n"
     ]
    }
   ],
   "source": [
    "#predecting and evaluating the model\n",
    "pred = dtrmodel.transform(test)\n",
    "print(pred.select('new_features',\"medv\",\"prediction\").show(5))\n",
    "evaluator=RegressionEvaluator(labelCol=\"medv\",predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "rmse=evaluator.evaluate(pred)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data is : %f\" %rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(13, {0: 0.0143, 4: 0.0234, 5: 0.2754, 6: 0.0014, 7: 0.0672, 9: 0.0106, 10: 0.0199, 11: 0.022, 12: 0.5659})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking checking important features for predicting the hosue prioces\n",
    "\n",
    "dtrmodel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(crim=0.00632, zn=18.0, indus=2.31, chas=0, nox=0.538, rm=6.575, age=65.2, dis=4.09, rad=1, tax=296, ptratio=15.3, b=396.9, lstat=4.98, medv=24.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|        new_features|medv|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[8.99180711494721...|31.6| 31.84496020519698|\n",
      "|[9.76735047861141...|50.0|49.017931489914545|\n",
      "|[1.28807636921618...|32.9|31.894822173009235|\n",
      "|[1.48252419807692...|33.0|32.566533486184916|\n",
      "|[1.49825986052808...|20.1|22.657167616523154|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#creating the model\n",
    "gbt=GBTRegressor(featuresCol='new_features',labelCol=\"medv\")\n",
    "gbtmodel=gbt.fit(train)\n",
    "#predicting\n",
    "pred=gbtmodel.transform(test)\n",
    "print(pred.select('new_features',\"medv\",\"prediction\").show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data is : 3.739767\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "evaluator=RegressionEvaluator(labelCol='medv',predictionCol='prediction',metricName='rmse')\n",
    "rmse=evaluator.evaluate(pred)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data is : %f\" %rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the above we notice that Gradient-boosted tree regression is the best performimg model since itn has the lowest Root Mean Squared Error(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
